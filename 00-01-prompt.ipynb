{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate()„ÅØGoogleGenerativeAI„ÅßChatGoogleGenerativeAI„Åß„ÅØ‰Ωø„Åà„Å™„ÅÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "\n",
    "llm = GoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "result = llm.generate(\n",
    "        ['Here is a fun fact about Pluto:',\n",
    "                     'Here is a fun fact about Mars:']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'LLMResult', 'description': 'A container for results of an LLM call.\\n\\nBoth chat models and LLMs generate an LLMResult object. This object contains\\nthe generated outputs and any additional information that the model provider\\nwants to return.', 'type': 'object', 'properties': {'generations': {'title': 'Generations', 'type': 'array', 'items': {'type': 'array', 'items': {'$ref': '#/definitions/Generation'}}}, 'llm_output': {'title': 'Llm Output', 'type': 'object'}, 'run': {'title': 'Run', 'type': 'array', 'items': {'$ref': '#/definitions/RunInfo'}}}, 'required': ['generations'], 'definitions': {'Generation': {'title': 'Generation', 'description': 'A single text generation output.\\n\\nGeneration represents the response from an \"old-fashioned\" LLM that\\ngenerates regular text (not chat messages).\\n\\nThis model is used internally by chat model and will eventually\\nbe mapped to a more general `LLMResult` object, and then projected into\\nan `AIMessage` object.\\n\\nLangChain users working with chat models will usually access information via\\n`AIMessage` (returned from runnable interfaces) or `LLMResult` (available\\nvia callbacks). Please refer the `AIMessage` and `LLMResult` schema documentation\\nfor more information.', 'type': 'object', 'properties': {'text': {'title': 'Text', 'type': 'string'}, 'generation_info': {'title': 'Generation Info', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'Generation', 'enum': ['Generation'], 'type': 'string'}}, 'required': ['text']}, 'RunInfo': {'title': 'RunInfo', 'description': 'Class that contains metadata for a single execution of a Chain or model.\\n\\nHere for backwards compatibility with older versions of langchain_core.\\n\\nThis model will likely be deprecated in the future.\\n\\nUsers can acquire the run_id information from callbacks or via run_id\\ninformation present in the astream_event API (depending on the use case).', 'type': 'object', 'properties': {'run_id': {'title': 'Run Id', 'type': 'string', 'format': 'uuid'}}, 'required': ['run_id']}}}\n",
      "LLMResult\n",
      "A container for results of an LLM call.\n",
      "\n",
      "Both chat models and LLMs generate an LLMResult object. This object contains\n",
      "the generated outputs and any additional information that the model provider\n",
      "wants to return.\n",
      "Generation\n",
      "A single text generation output.\n",
      "\n",
      "Generation represents the response from an \"old-fashioned\" LLM that\n",
      "generates regular text (not chat messages).\n",
      "\n",
      "This model is used internally by chat model and will eventually\n",
      "be mapped to a more general `LLMResult` object, and then projected into\n",
      "an `AIMessage` object.\n",
      "\n",
      "LangChain users working with chat models will usually access information via\n",
      "`AIMessage` (returned from runnable interfaces) or `LLMResult` (available\n",
      "via callbacks). Please refer the `AIMessage` and `LLMResult` schema documentation\n",
      "for more information.\n"
     ]
    }
   ],
   "source": [
    "print(result.schema())\n",
    "print(result.schema()[\"title\"])\n",
    "print(result.schema()[\"description\"])\n",
    "print(result.schema()[\"definitions\"][\"Generation\"][\"title\"])\n",
    "print(result.schema()[\"definitions\"][\"Generation\"][\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text=\"Please tell me the fun fact about Pluto! I'm eager to learn something new about this fascinating dwarf planet. üòÑ \\n\")], [Generation(text=\"Please tell me the fun fact about Mars! I'm eager to learn something new about the Red Planet. üòä \\n\")]] llm_output=None run=[RunInfo(run_id=UUID('422e9e1a-00c6-43d1-944f-1df8b12ae7a7')), RunInfo(run_id=UUID('4a828f04-0af3-4d13-9cc8-1c0b89c3dce8'))]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(result.generations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please tell me the fun fact about Pluto! I'm eager to learn something new about this fascinating dwarf planet. üòÑ \n",
      "\n",
      "Please tell me the fun fact about Mars! I'm eager to learn something new about the Red Planet. üòä \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result.generations[0][0].text)\n",
    "print(result.generations[1][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a fact'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# An example prompt with no input variables\n",
    "no_input_prompt = PromptTemplate(input_variables=[], template=\"Tell me a fact\")\n",
    "no_input_prompt.format()\n",
    "# -> \"Tell me a fact.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a fact about Mars.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example prompt with one input variable\n",
    "one_input_prompt = PromptTemplate(input_variables=[\"topic\"], template=\"Tell me a fact about {topic}.\")\n",
    "# Notice how the stirng \"topic\" gets automatically converted to a parameter name, very convienent! \n",
    "one_input_prompt.format(topic=\"Mars\")\n",
    "# -> \"Tell me a fact about Mars\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a fact about Mars for a student 8th Grade level.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example prompt with multiple input variables\n",
    "multiple_input_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"level\"], \n",
    "    template=\"Tell me a fact about {topic} for a student {level} level.\"\n",
    ")\n",
    "multiple_input_prompt.format(topic='Mars',level='8th Grade')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- „Åæ„Åöimport\n",
    "- system_template„Åß\"you are an ai recipe assitant   \"„Åß{dietary_preference}„Å®{cooking_time}„Çí„Éë„É©„É°„Éº„Çø„Åß„Çª„ÉÉ„Éà\n",
    "- Ê¨°„Å´human_template=\"{recipe_request}\"\n",
    "- cooking_time=\"15 min\", dietary_preference=\"Vegan\",  receipe_request=\"Quick Snack\"\n",
    "- chat_prompt = ChatPromptTemplate.from_message\n",
    "\n",
    "  https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html\n",
    "  Create a chat prompt template from a variety of message formats.\n",
    "\n",
    "- request = chat_prompt.format_prompt„ÅßCreate Prompt Value.\n",
    "- „É¢„Éá„É´„Å´rquest„ÇíÊ∏°„Åô\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template=\"You are an AI recipe assistant that specializes in {dietary_preference} dishes that can be prepared in {cooking_time}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cooking_time', 'dietary_preference']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message_prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_template=\"{recipe_request}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['recipe_request']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_message_prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cooking_time', 'dietary_preference', 'recipe_request']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are an AI recipe assistant that specializes in Vegan dishes that can be prepared in 15 min.'),\n",
       " HumanMessage(content='Quick Snack')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a chat completion from the formatted messages\n",
    "chat_prompt.format_prompt(cooking_time=\"15 min\", dietary_preference=\"Vegan\", recipe_request=\"Quick Snack\").to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = chat_prompt.format_prompt(cooking_time=\"15 min\", dietary_preference=\"Vegan\", recipe_request=\"Quick Snack\").to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "chat = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "# chat.invoke(\"tell me about gold price\")       \n",
    "result = chat.invoke(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 5-Minute Vegan Hummus Toast\n",
      "\n",
      "**This is a super quick and easy snack that's packed with flavor and nutrients.**\n",
      "\n",
      "**Ingredients:**\n",
      "\n",
      "* 2 slices of whole-wheat bread\n",
      "* 2 tablespoons hummus\n",
      "* 1/4 avocado, sliced\n",
      "* 1/4 cup cherry tomatoes, halved\n",
      "* 1/4 teaspoon dried oregano\n",
      "* Pinch of salt and pepper\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "1. Toast the bread until golden brown.\n",
      "2. Spread hummus on both slices of toast.\n",
      "3. Top with avocado slices, cherry tomatoes, oregano, salt, and pepper.\n",
      "4. Enjoy!\n",
      "\n",
      "**Variations:**\n",
      "\n",
      "* Add a sprinkle of red pepper flakes for a spicy kick.\n",
      "* Use different toppings like cucumber, red onion, or spinach.\n",
      "* Spread with a different dip like pesto or baba ghanoush. \n",
      "\n",
      "**Tip:** Make this a meal by adding a side salad or fruit. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".py312env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
